Gradient descent is a method for algorithm optimisation which uses the [[Derivative]] of the cost formula to find the optimal changes which can be incrementally implemented into the weights and biases of the algorithm.
![[Gradient Descent Example.png]]

## How does gradient descent work for algorithms?
Gradient descent is used to optimise the entire algorithm, not just the single parameter which is being updated. Its important to note that when there are multiple features in a problem, the weights are each updated with an independent value but all the independent values point towards a local minima that will optimise ALL values of a function not just the value which is being updated. This is why it is important that the cost function takes into account all previous weights when updating a single weight parameter.