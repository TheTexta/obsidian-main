Gradient descent is a method for algorithm optimization which uses the [[Derivative]] of the cost formula to find the optimal changes which can be incrementally implemented into the [[Weights and Biases]] of the algorithm.
![[Gradient Descent Example.png]]

## How does gradient descent work for algorithms?
Gradient descent is used to optimize the entire algorithm, not just the single [[Weights and Biases|weight/biase]] which is being updated. Its important to note that when there are multiple [[features vs examples]] in a problem, the weights are each updated with an independent value but all the independent values point towards a local minima that will optimize ALL values of a function not just the value which is being updated. This is why it is important that the cost function takes into account all previous [[Weights and Biases|weights]] when updating a single weight parameter.